# -*- coding: utf-8 -*-
"""Copie de PEA23-NeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qCn_BoUjMZ3TQMli56j7GPm4Ch2IVCBK

Import des différentes librairies nécessaires
"""

import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.regularizers import l1, l2
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization

"""Chargement et prétraitement des données"""

data = pd.read_csv('blog/programs/data.csv')

# Supposons que la première colonne s'appelle 'description'
texts = data['colonne_texte'].values
labels = data.iloc[:, 1:].values  # Prendre toutes les colonnes après la description

"""Tokenisation des textes"""

MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 250
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(texts)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

"""Création du modèle"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.3))
model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(7, activation='sigmoid', kernel_regularizer=l2(0.01)))
model.add(BatchNormalization())
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

"""Séparation des données en ensemble entrainement/texte"""

X_train, X_test, Y_train, Y_test = train_test_split(X, labels, test_size=0.35, random_state=42)

"""Entrainement du modèle"""

epochs = 300
batch_size = 64



early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)
model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop])

"""Evaluation du modèle"""

accuracy = model.evaluate(X_test, Y_test)
print("Test Set:\n Loss: {:0.3f}\n Accuracy: {:0.3f}".format(accuracy[0], accuracy[1]))

"""
________________________________________________________________________________
--------------------------------------------------------------------------------
________________________________________________________________________________

POur tester le modèle, remplacer le contenu de "texte_input"
"""

def preprocess_text(text):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
    return padded_sequence

texte_input = "Le robot inspectera des pièces"
input_data = preprocess_text(texte_input)

predicted_labels = model.predict(input_data)

capteur_names = ["Caméra RGB", "Caméra Infrarouge", "Caméra Thermique", "Caméra de profondeur", "Capteurs Ultrasons", "LED", "Laster télémètre"]  # Remplissez ceci avec les noms réels des capteurs
threshold = 0.2

predicted_capteurs = [capteur_names[i] for i, score in enumerate(predicted_labels[0]) if score > threshold]

print("Capteurs prédits :", ", ".join(predicted_capteurs))
model.save_weights("blog/programs/model_weights.h5")
